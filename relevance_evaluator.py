"""
Relevance Evaluator - LLMæ‰¹é‡è¯„ä¼°chunkç›¸å…³æ€§
ç”¨äºå…¨æ–‡æ¡£æ£€ç´¢çš„æ™ºèƒ½è¿‡æ»¤
"""
import asyncio
from typing import List, Dict, Tuple
from loguru import logger
from langchain.schema import Document


class RelevanceEvaluator:
    """LLMç›¸å…³æ€§è¯„ä¼°å™¨"""

    def __init__(self, llm_client_manager):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            llm_client_manager: LLMå®¢æˆ·ç«¯ç®¡ç†å™¨
        """
        self.llm_manager = llm_client_manager
        self.evaluation_cache = {}  # ç¼“å­˜è¯„ä¼°ç»“æœ

    async def batch_evaluate_relevance(
        self,
        question: str,
        chunks: List[Document],
        batch_size: int = 50
    ) -> Tuple[List[Document], Dict]:
        """
        æ‰¹é‡è¯„ä¼°chunksç›¸å…³æ€§

        Args:
            question: ç”¨æˆ·é—®é¢˜
            chunks: å¾…è¯„ä¼°çš„chunksåˆ—è¡¨
            batch_size: æ‰¹é‡å¤§å°

        Returns:
            (ç›¸å…³chunks, è¯„ä¼°ç»Ÿè®¡)
        """
        if not chunks:
            return [], {"total": 0, "relevant": 0, "irrelevant": 0}

        logger.info(f"ğŸ§  å¼€å§‹LLMæ‰¹é‡è¯„ä¼° {len(chunks)} ä¸ªchunksçš„ç›¸å…³æ€§")

        relevant_chunks = []
        stats = {
            "total": len(chunks),
            "relevant": 0,
            "irrelevant": 0,
            "cached": 0
        }

        # åˆ†æ‰¹è¯„ä¼°
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]
            logger.info(f"  è¯„ä¼°æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} ä¸ªchunks")

            # æ‰¹é‡è¯„ä¼°å½“å‰æ‰¹æ¬¡
            batch_results = await self._evaluate_batch(question, batch)

            # æ”¶é›†ç›¸å…³çš„chunks
            for chunk, is_relevant, from_cache in batch_results:
                if is_relevant:
                    relevant_chunks.append(chunk)
                    stats["relevant"] += 1
                else:
                    stats["irrelevant"] += 1

                if from_cache:
                    stats["cached"] += 1

        logger.info(f"âœ… è¯„ä¼°å®Œæˆ: {stats['relevant']}/{stats['total']} ä¸ªç›¸å…³chunks")
        logger.info(f"  ç¼“å­˜å‘½ä¸­: {stats['cached']}/{stats['total']}")

        return relevant_chunks, stats

    async def _evaluate_batch(
        self,
        question: str,
        batch: List[Document]
    ) -> List[Tuple[Document, bool, bool]]:
        """
        è¯„ä¼°ä¸€æ‰¹chunks

        Returns:
            [(chunk, is_relevant, from_cache), ...]
        """
        results = []

        # å¹¶å‘è¯„ä¼°æ¯ä¸ªchunk
        tasks = [
            self._evaluate_single(question, chunk)
            for chunk in batch
        ]

        evaluations = await asyncio.gather(*tasks)

        for chunk, is_relevant, from_cache in zip(batch, evaluations, [False]*len(batch)):
            results.append((chunk, is_relevant, from_cache))

        return results

    async def _evaluate_single(
        self,
        question: str,
        chunk: Document
    ) -> bool:
        """
        è¯„ä¼°å•ä¸ªchunkæ˜¯å¦ç›¸å…³

        Returns:
            True if relevant, False otherwise
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(question, chunk)
        if cache_key in self.evaluation_cache:
            return self.evaluation_cache[cache_key]

        # æ„å»ºè¯„ä¼°prompt
        prompt = self._build_evaluation_prompt(question, chunk)

        try:
            # è°ƒç”¨LLM
            response = await asyncio.to_thread(
                self.llm_manager.client.invoke,
                prompt
            )

            # è§£æç»“æœ
            output = response.content.strip().upper()
            is_relevant = self._parse_relevance(output)

            # ç¼“å­˜ç»“æœ
            self.evaluation_cache[cache_key] = is_relevant

            return is_relevant

        except Exception as e:
            logger.error(f"è¯„ä¼°chunkå¤±è´¥: {e}")
            # å¤±è´¥æ—¶é»˜è®¤ç›¸å…³ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            return True

    def _build_evaluation_prompt(self, question: str, chunk: Document) -> str:
        """æ„å»ºè¯„ä¼°prompt"""
        content = chunk.page_content[:500]  # é™åˆ¶é•¿åº¦

        prompt = f"""è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µæ˜¯å¦ä¸é—®é¢˜ç›¸å…³ã€‚

**é—®é¢˜**: {question}

**æ–‡æœ¬ç‰‡æ®µ**:
{content}

**åˆ¤æ–­æ ‡å‡†**:
- å¦‚æœæ–‡æœ¬ç›´æ¥å›ç­”æˆ–éƒ¨åˆ†å›ç­”äº†é—®é¢˜ï¼Œå›ç­”"Y"
- å¦‚æœæ–‡æœ¬æä¾›äº†ç›¸å…³èƒŒæ™¯æˆ–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”"Y"  
- å¦‚æœæ–‡æœ¬ä¸é—®é¢˜å®Œå…¨æ— å…³ï¼Œå›ç­”"N"

**è¯·ä»…å›ç­” Y (ç›¸å…³) æˆ– N (ä¸ç›¸å…³)ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹**:"""

        return prompt

    @staticmethod
    def _parse_relevance(output: str) -> bool:
        """è§£æLLMè¾“å‡º"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«Y
        if output.startswith("Y") or "ç›¸å…³" in output:
            return True
        # æ£€æŸ¥æ˜¯å¦åŒ…å«N
        elif output.startswith("N") or "ä¸ç›¸å…³" in output:
            return False
        else:
            # é»˜è®¤ç›¸å…³ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            logger.warning(f"æ— æ³•è§£æLLMè¾“å‡º: {output}ï¼Œé»˜è®¤ä¸ºç›¸å…³")
            return True

    @staticmethod
    def _get_cache_key(question: str, chunk: Document) -> str:
        """ç”Ÿæˆç¼“å­˜key"""
        # ä½¿ç”¨é—®é¢˜å’Œchunkå†…å®¹çš„hashä½œä¸ºkey
        content_hash = hash(chunk.page_content[:200])
        question_hash = hash(question)
        return f"{question_hash}_{content_hash}"

    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self.evaluation_cache.clear()
        logger.info("å·²æ¸…é™¤è¯„ä¼°ç¼“å­˜")