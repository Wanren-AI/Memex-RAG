"""
FastAPI Backend Server
æä¾›RESTful APIæ¥å£ä¾›å‰ç«¯è°ƒç”¨
"""
from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import uvicorn
import tempfile
import os
import sys
from pathlib import Path
from loguru import logger
import json
import asyncio

# Add project path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from rag_system import DocumentAssistant
from config import AvailableModels

# Initialize FastAPI app
app = FastAPI(
    title="RAG Document Assistant API",
    description="æ™ºèƒ½æ–‡æ¡£åˆ†æåŠ©æ‰‹APIæœåŠ¡",
    version="1.0.0"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global assistant instance
assistant: Optional[DocumentAssistant] = None


# ========== Pydantic Models ==========

class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    message: str = Field(..., description="ç”¨æˆ·æ¶ˆæ¯")
    document_name: Optional[str] = Field(None, description="æ–‡æ¡£åç§°")
    model: Optional[str] = Field(None, description="æ¨¡å‹åç§°")
    max_tokens: Optional[int] = Field(None, description="æœ€å¤§tokenæ•°")
    temperature: Optional[float] = Field(None, description="æ¸©åº¦å‚æ•°")
    stream: bool = Field(True, description="æ˜¯å¦æµå¼è¾“å‡º")


class AllDocsRequest(BaseModel):
    """å…¨æ–‡æ¡£æŸ¥è¯¢è¯·æ±‚"""
    message: str = Field(..., description="ç”¨æˆ·æ¶ˆæ¯")
    search_mode: str = Field("fast", description="æ£€ç´¢æ¨¡å¼: fastæˆ–smart")
    top_k: int = Field(10, description="æ£€ç´¢æ•°é‡")
    fallback_ratio: float = Field(0.5, description="ä¿åº•æ¯”ä¾‹")
    stream: bool = Field(True, description="æ˜¯å¦æµå¼è¾“å‡º")


class ModelUpdateRequest(BaseModel):
    """æ¨¡å‹æ›´æ–°è¯·æ±‚"""
    model_name: str = Field(..., description="æ¨¡å‹åç§°")


class ParametersUpdateRequest(BaseModel):
    """å‚æ•°æ›´æ–°è¯·æ±‚"""
    temperature: Optional[float] = Field(None, description="æ¸©åº¦")
    max_tokens: Optional[int] = Field(None, description="æœ€å¤§tokenæ•°")


class DocumentUpdateRequest(BaseModel):
    """æ–‡æ¡£æ›´æ–°è¯·æ±‚"""
    force: bool = Field(False, description="å¼ºåˆ¶æ›´æ–°")


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”"""
    answer: str = Field(..., description="å›ç­”å†…å®¹")
    sources: List[Dict[str, Any]] = Field(default_factory=list, description="å¼•ç”¨æ¥æº")
    done: bool = Field(False, description="æ˜¯å¦å®Œæˆ")
    metadata: Optional[Dict[str, Any]] = Field(None, description="å…ƒæ•°æ®")


class DocumentInfo(BaseModel):
    """æ–‡æ¡£ä¿¡æ¯"""
    name: str
    size_mb: float
    modified_time: str
    chunk_count: int
    file_hash: str
    indexed: bool


class StatusResponse(BaseModel):
    """ç³»ç»ŸçŠ¶æ€å“åº”"""
    status: str
    model: str
    documents: int
    conversation_turns: int
    retrieval_config: Dict[str, Any]


class MessageResponse(BaseModel):
    """é€šç”¨æ¶ˆæ¯å“åº”"""
    success: bool
    message: str
    data: Optional[Any] = None


# ========== Lifecycle Events ==========

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    global assistant
    logger.info("ğŸš€ Starting RAG API Server...")

    # Check API key
    if not os.getenv("DASHSCOPE_API_KEY"):
        logger.error("DASHSCOPE_API_KEY not set!")
        raise RuntimeError("DASHSCOPE_API_KEY environment variable is required")

    # Initialize assistant
    try:
        assistant = DocumentAssistant()
        logger.info("âœ… DocumentAssistant initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize DocumentAssistant: {e}")
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶æ¸…ç†"""
    logger.info("ğŸ‘‹ Shutting down RAG API Server...")


# ========== Health Check ==========

@app.get("/", tags=["System"])
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "RAG Document Assistant API",
        "version": "1.0.0",
        "docs": "/docs"
    }


@app.get("/health", tags=["System"])
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}


@app.get("/status", response_model=StatusResponse, tags=["System"])
async def get_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    if not assistant:
        raise HTTPException(status_code=500, detail="Assistant not initialized")

    status_info = assistant.get_status()
    return StatusResponse(
        status="running",
        model=status_info["model"],
        documents=status_info["documents"],
        conversation_turns=status_info["conversation_turns"],
        retrieval_config=status_info["retrieval_config"]
    )


# ========== Document Management ==========

@app.get("/documents", tags=["Documents"])
async def list_documents() -> List[str]:
    """è·å–æ–‡æ¡£åˆ—è¡¨"""
    if not assistant:
        raise HTTPException(status_code=500, detail="Assistant not initialized")

    return assistant.list_documents()


@app.get("/documents/{document_name}", response_model=DocumentInfo, tags=["Documents"])
async def get_document_info(document_name: str):
    """è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯"""
    if not assistant:
        raise HTTPException(status_code=500, detail="Assistant not initialized")

    info = assistant.get_document_info(document_name)
    if not info:
        raise HTTPException(status_code=404, detail=f"Document not found: {document_name}")

    return DocumentInfo(**info)


@app.post("/documents/upload", response_model=MessageResponse, tags=["Documents"])
async def upload_document(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡æ¡£"""
    if not assistant:
        raise HTTPException(status_code=500, detail="Assistant not initialized")

    # Save uploaded file to temp location
    try:
        # ç¡®ä¿æ–‡ä»¶åæ­£ç¡®ç¼–ç 
        original_filename = file.filename
        if isinstance(original_filename, bytes):
            original_filename = original_filename.decode('utf-8')

        logger.info(f"Uploading file: {original_filename}")

        suffix = Path(original_filename).suffix
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            content = await file.read()
            tmp.write(content)
            tmp_path = tmp.name

        # Upload to assistant (ä¼ é€’åŸå§‹æ–‡ä»¶å)
        result = assistant.upload_document(tmp_path, original_filename=original_filename)

        # Clean up temp file
        os.unlink(tmp_path)

        if result:
            return MessageResponse(
                success=True,
                message=f"Document uploaded successfully: {original_filename}",
                data={"filename": original_filename}
            )
        else:
            raise HTTPException(status_code=400, detail="Failed to upload document")

    except Exception as e:
        logger.error(f"Upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.put("/documents/{document_name}", response_model=MessageResponse, tags=["Documents"])
async def update_document(
    document_name: str,
    file: UploadFile = File(...),
    force: bool = False
):
    """æ›´æ–°æ–‡æ¡£"""
    if not assistant:
        raise HTTPException(status_code=500, detail="Assistant not initialized")

    # Validate filename match
    if file.filename != document_name:
        raise HTTPException(
            status_code=400,
            detail=f"Filename mismatch: {file.filename} != {document_name}"
        )

    try:
        # Save to temp file
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as tmp:
            content = await file.read()
            tmp.write(content)
            tmp_path = tmp.name

        # Update document
        result = assistant.update_document(tmp_path, force=force)

        # Clean up
        os.unlink(tmp_path)

        if result:
            return MessageResponse(
                success=True,
                message=f"Document updated successfully: {document_name}",
                data={"filename": document_name}
            )
        else:
            raise HTTPException(status_code=400, detail="Failed to update document")

    except Exception as e:
        logger.error(f"Update error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/documents/{document_name}", response_model=MessageResponse, tags=["Documents"])
async def delete_document(document_name: str):
    """åˆ é™¤æ–‡æ¡£"""
    if not assistant:
        raise HTTPException(status_code=500, detail="Assistant not initialized")

    result = assistant.delete_document(document_name)

    if result:
        return MessageResponse(
            success=True,
            message=f"Document deleted successfully: {document_name}",
            data={"filename": document_name}
        )
    else:
        raise HTTPException(status_code=404, detail=f"Document not found: {document_name}")


# ========== Chat / Query ==========

@app.post("/chat", tags=["Chat"])
async def chat(request: ChatRequest):
    """èŠå¤©æ¥å£ï¼ˆæ”¯æŒæµå¼å’Œéæµå¼ï¼‰"""
    if not assistant:
        raise HTTPException(status_code=500, detail="Assistant not initialized")

    # Update model if specified
    if request.model:
        assistant.switch_model(request.model)

    # Update parameters if specified
    if request.temperature is not None or request.max_tokens is not None:
        assistant.update_parameters(
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )

    # Handle document selection
    doc_name = None
    if request.document_name and request.document_name not in ["ä¸ä½¿ç”¨çŸ¥è¯†åº“", "None"]:
        doc_name = request.document_name

    # Stream or non-stream response
    if request.stream:
        async def generate():
            try:
                for chunk in assistant.ask_stream(request.message, doc_name):
                    if chunk:
                        # ç¡®ä¿æ¯ä¸ªå­—ç¬¦éƒ½ç«‹å³å‘é€
                        data = {"answer": chunk, "done": False}
                        yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                        # å¼ºåˆ¶åˆ·æ–°ï¼Œç¡®ä¿ç«‹å³å‘é€
                        await asyncio.sleep(0)

                # Send done signal
                yield f"data: {json.dumps({'answer': '', 'done': True}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logger.error(f"Stream error: {e}")
                yield f"data: {json.dumps({'error': str(e), 'done': True}, ensure_ascii=False)}\n\n"

        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no"
            }
        )
    else:
        # Non-streaming response
        answer = assistant.ask(request.message, doc_name)
        return ChatResponse(answer=answer, sources=[], done=True)


@app.post("/chat/all-documents", tags=["Chat"])
async def chat_all_documents(request: AllDocsRequest):
    """å…¨æ–‡æ¡£æŸ¥è¯¢æ¥å£"""
    if not assistant:
        raise HTTPException(status_code=500, detail="Assistant not initialized")

    if request.stream:
        async def generate():
            if request.search_mode == "smart":
                # Smart mode with LLM filtering
                # ä½¿ç”¨åŒæ­¥æ–¹å¼è°ƒç”¨ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
                import threading
                import queue

                result_queue = queue.Queue()
                error_queue = queue.Queue()

                def run_smart_stream():
                    try:
                        for chunk in assistant.ask_all_documents_smart_stream(
                            question=request.message,
                            top_k=request.top_k,
                            fallback_ratio=request.fallback_ratio
                        ):
                            result_queue.put(chunk)
                        result_queue.put(None)  # ç»“æŸä¿¡å·
                    except Exception as e:
                        error_queue.put(e)
                        result_queue.put(None)

                # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œ
                thread = threading.Thread(target=run_smart_stream)
                thread.start()

                while True:
                    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                    if not error_queue.empty():
                        error = error_queue.get()
                        yield f"data: {json.dumps({'error': str(error), 'done': True}, ensure_ascii=False)}\n\n"
                        break

                    # è·å–ç»“æœ
                    chunk = result_queue.get()
                    if chunk is None:
                        break

                    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0)  # ç¡®ä¿ç«‹å³å‘é€

                thread.join()
            else:
                # Fast mode - ä¸´æ—¶ç¦ç”¨reranké¿å…SSLé”™è¯¯
                original_rerank = assistant.config.retrieval.use_rerank
                try:
                    # ä¸´æ—¶ç¦ç”¨rerank
                    assistant.config.retrieval.use_rerank = False
                    logger.info("å¿«é€Ÿæ¨¡å¼ï¼šå·²ä¸´æ—¶ç¦ç”¨rerankä»¥é¿å…SSLé”™è¯¯")

                    for chunk in assistant.ask_all_documents_stream(request.message):
                        yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                        await asyncio.sleep(0)
                finally:
                    # æ¢å¤åŸå§‹è®¾ç½®
                    assistant.config.retrieval.use_rerank = original_rerank

        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no"
            }
        )
    else:
        # Non-streaming
        if request.search_mode == "smart":
            # Smart mode doesn't have non-streaming version
            raise HTTPException(
                status_code=400,
                detail="Smart mode only supports streaming"
            )
        else:
            result = assistant.ask_all_documents(request.message)
            return result


@app.post("/chat/clear", response_model=MessageResponse, tags=["Chat"])
async def clear_conversation():
    """æ¸…é™¤å¯¹è¯å†å²"""
    if not assistant:
        raise HTTPException(status_code=500, detail="Assistant not initialized")

    assistant.clear_conversation()
    return MessageResponse(
        success=True,
        message="Conversation history cleared"
    )


@app.get("/chat/history", tags=["Chat"])
async def get_conversation_history():
    """è·å–å¯¹è¯å†å²"""
    if not assistant:
        raise HTTPException(status_code=500, detail="Assistant not initialized")

    history = assistant.get_conversation_history()
    return {"history": [{"role": msg.type, "content": msg.content} for msg in history]}


# ========== Model Management ==========

@app.get("/models", tags=["Models"])
async def list_models() -> List[str]:
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return AvailableModels.all()


@app.get("/models/current", tags=["Models"])
async def get_current_model() -> Dict[str, str]:
    """è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
    if not assistant:
        raise HTTPException(status_code=500, detail="Assistant not initialized")

    return {"model": assistant.get_current_model()}


@app.put("/models/switch", response_model=MessageResponse, tags=["Models"])
async def switch_model(request: ModelUpdateRequest):
    """åˆ‡æ¢æ¨¡å‹"""
    if not assistant:
        raise HTTPException(status_code=500, detail="Assistant not initialized")

    result = assistant.switch_model(request.model_name)

    if result:
        return MessageResponse(
            success=True,
            message=f"Switched to model: {request.model_name}",
            data={"model": request.model_name}
        )
    else:
        raise HTTPException(
            status_code=400,
            detail=f"Failed to switch to model: {request.model_name}"
        )


@app.put("/models/parameters", response_model=MessageResponse, tags=["Models"])
async def update_parameters(request: ParametersUpdateRequest):
    """æ›´æ–°æ¨¡å‹å‚æ•°"""
    if not assistant:
        raise HTTPException(status_code=500, detail="Assistant not initialized")

    assistant.update_parameters(
        temperature=request.temperature,
        max_tokens=request.max_tokens
    )

    return MessageResponse(
        success=True,
        message="Parameters updated",
        data={
            "temperature": request.temperature,
            "max_tokens": request.max_tokens
        }
    )


# ========== Error Handlers ==========

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """é€šç”¨å¼‚å¸¸å¤„ç†"""
    logger.error(f"Unhandled exception: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "success": False,
            "message": "Internal server error",
            "detail": str(exc)
        }
    )


def main():
    """å¯åŠ¨APIæœåŠ¡å™¨"""
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    )


if __name__ == "__main__":
    main()