"""
Conversation Management Module
Handles chat interactions with history management
å¯¹è¯ç®¡ç† äº¤äº’æ§åˆ¶

"""
from typing import Iterator, Optional, Dict, Any, List
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain_core.messages import AIMessageChunk
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import AddableDict
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from loguru import logger

from llm_client import LLMClientManager
from knowledge_base import KnowledgeBaseManager
from relevance_evaluator import RelevanceEvaluator


class ConversationManager:
    """
    Manages conversational interactions
    Supports both knowledge-based and general chat
    """

    # System prompts æœ‰æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼Œå¼ºè°ƒ"åŸºäºæ–‡æ¡£å›ç­”"
    KB_SYSTEM_PROMPT = """You are a professional knowledge assistant.
Use the retrieved context to answer questions. If you don't know the answer, 
clearly state that you couldn't find the information.

Context:
{context}
"""
    #æ— æ–‡æ¡£ï¼Œè‡ªç”±å¯¹è¯
    GENERAL_SYSTEM_PROMPT = """You are a helpful assistant that answers 
various questions to the best of your ability."""

    MAX_HISTORY_TURNS = 3  # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯ï¼ˆ6æ¡æ¶ˆæ¯

    def __init__(
        self,
        llm_manager: LLMClientManager,
        kb_manager: KnowledgeBaseManager
    ):
        """
        Initialize conversation manager

        Args:
            llm_manager: LLM client manager
            kb_manager: Knowledge base manager
        """
        self.llm_manager = llm_manager
        self.kb_manager = kb_manager
        self.chat_history = ChatMessageHistory()

        # Initialize relevance evaluator for smart mode
        self.relevance_evaluator = RelevanceEvaluator(llm_manager)

        # Create prompts
        self.kb_prompt = ChatPromptTemplate.from_messages([
            ("system", self.KB_SYSTEM_PROMPT),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ])

        self.general_prompt = ChatPromptTemplate.from_messages([
            ("system", self.GENERAL_SYSTEM_PROMPT),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ])

    def _manage_history(self) -> None:
        """Manage conversation history to prevent context overflow"""
        """é˜²æ­¢ä¸Šä¸‹æ–‡è¿‡é•¿"""
        max_messages = self.MAX_HISTORY_TURNS * 2
        if len(self.chat_history.messages) >= max_messages:
            self.chat_history.messages = self.chat_history.messages[-max_messages:]
            logger.debug(f"History trimmed to {max_messages} messages")

    def _build_chain(self, document_name: Optional[str]):
        """
        Build conversation chain based on context

        Args:
            document_name: Optional document for knowledge-based chat

        Returns:
            Runnable chain with history
        """
        self._manage_history()

        llm_client = self.llm_manager.client

        if document_name:
            # Knowledge-based chain
            retriever = self.kb_manager.get_retriever(document_name)

            if retriever is None:
                logger.warning(f"Retriever not found for: {document_name}")
                chain = self.general_prompt | llm_client | self._streaming_parser
            else:
                qa_chain = create_stuff_documents_chain(llm_client, self.kb_prompt)
                chain = create_retrieval_chain(retriever, qa_chain)
                logger.info("Knowledge-based chain created")
        else:
            # General conversation chain
            chain = self.general_prompt | llm_client | self._streaming_parser
            logger.info("General conversation chain created")

        # Wrap with history
        chain_with_history = RunnableWithMessageHistory(
            chain,
            lambda session_id: self.chat_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )

        return chain_with_history

    @staticmethod
    def _streaming_parser(chunks: Iterator[AIMessageChunk]) -> Iterator[Dict]:
        """
        æµå¼è¿”å›ç­”æ¡ˆï¼Œè¾¹ç”Ÿæˆè¾¹è¾“å‡º
        Parse streaming chunks

        Args:
            chunks: Stream of AI message chunks

        Yields:
            Dictionaries with answer content
        """
        for chunk in chunks:
            yield AddableDict({'answer': chunk.content})#åˆ†æ‰¹è¿”å›æ­¤å¯¹è±¡

    def chat(
        self,
        question: str,
        document_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Synchronous chat interaction

        Args:
            question: User question
            document_name: Optional document for context

        Returns:
            Response dictionary
        """
        chain = self._build_chain(document_name)

        response = chain.invoke(
            {"input": question},
            {"configurable": {"session_id": "default"}}
        )

        logger.info(f"Chat completed for question: {question[:50]}...")
        return response

    def chat_stream(
        self,
        question: str,
        document_name: Optional[str] = None
    ) -> Iterator[Dict[str, Any]]:
        """
        Streaming chat interaction

        Args:
            question: User question
            document_name: Optional document for context

        Yields:
            Response chunks
        """
        chain = self._build_chain(document_name)

        logger.info(f"Starting stream for: {question[:50]}...")

        for chunk in chain.stream(
            {"input": question},
            {"configurable": {"session_id": "default"}}
        ):
            yield chunk

    def clear_history(self) -> None:
        """Clear conversation history"""
        self.chat_history.clear()
        logger.info("Conversation history cleared")

    def get_history(self) -> list:
        """
        Get conversation history

        Returns:
            List of messages
        """
        return self.chat_history.messages

    def get_history_summary(self) -> Dict[str, int]:
        """
        Get history statistics

        Returns:
            Dictionary with history stats
        """
        return {
            "total_messages": len(self.chat_history.messages),
            "conversation_turns": len(self.chat_history.messages) // 2
        }

    async def smart_all_documents_retrieve(
        self,
        question: str,
        all_docs: List[str],
        top_k: int = 10,
        fallback_ratio: float = 0.5
    ) -> Dict[str, Any]:
        """
        æ™ºèƒ½æ¨¡å¼å…¨æ–‡æ¡£æ£€ç´¢ï¼ˆå¸¦LLMç›¸å…³æ€§è¿‡æ»¤ï¼‰

        å·¥ä½œæµç¨‹ï¼š
        1. ä»æ‰€æœ‰æ–‡æ¡£å‘é‡æ£€ç´¢Top-K chunks
        2. ç”¨LLMæ‰¹é‡è¯„ä¼°æ¯ä¸ªchunkæ˜¯å¦ç›¸å…³
        3. å¦‚æœæ²¡æœ‰ç›¸å…³chunkï¼Œåˆ™rerankåå–å‰K*fallback_ratioä¸ª

        Args:
            question: ç”¨æˆ·é—®é¢˜
            all_docs: æ‰€æœ‰æ–‡æ¡£ååˆ—è¡¨
            top_k: å‘é‡æ£€ç´¢çš„top-k
            fallback_ratio: ä¿åº•æ¯”ä¾‹

        Returns:
            {
                'relevant_chunks': List[Document],
                'sources': List[Dict],
                'stats': Dict,
                'fallback_used': bool
            }
        """
        logger.info(f"ğŸ§  æ™ºèƒ½æ¨¡å¼å…¨æ–‡æ¡£æ£€ç´¢: {len(all_docs)} ä¸ªæ–‡æ¡£, top_k={top_k}")

        # Step 1: ä»æ‰€æœ‰æ–‡æ¡£æ”¶é›†chunksï¼ˆå¸¦åˆ†æ•°ï¼‰
        all_chunks_with_scores = []  # å­˜å‚¨ (chunk, score, doc_name)

        for doc_name in all_docs:
            retriever = self.kb_manager.get_retriever(doc_name)
            if retriever:
                try:
                    # è·å–å¸¦åˆ†æ•°çš„æ£€ç´¢ç»“æœ
                    docs_with_scores = retriever.get_relevant_documents(question)

                    # ä¸ºæ¯ä¸ªchunkæ·»åŠ æ¥æºä¿¡æ¯
                    for i, doc in enumerate(docs_with_scores):
                        # ä½¿ç”¨æ£€ç´¢æ’åä½œä¸ºåˆ†æ•°ï¼ˆæ’åè¶Šé å‰åˆ†æ•°è¶Šé«˜ï¼‰
                        score = 1.0 / (i + 1)  # ç¬¬1ä¸ªå¾—åˆ†1.0ï¼Œç¬¬2ä¸ª0.5ï¼Œç¬¬3ä¸ª0.33...
                        all_chunks_with_scores.append((doc, score, doc_name))

                    logger.info(f"  ä» {doc_name} æ£€ç´¢åˆ° {len(docs_with_scores)} ä¸ªchunks")
                except Exception as e:
                    logger.error(f"  ä» {doc_name} æ£€ç´¢å¤±è´¥: {e}")

        if not all_chunks_with_scores:
            return {
                'relevant_chunks': [],
                'sources': [],
                'stats': {'total': 0, 'relevant': 0, 'irrelevant': 0},
                'fallback_used': False
            }

        # Step 2: æŒ‰åˆ†æ•°æ’åºï¼Œåªå–å‰top_kä¸ªè¿›è¡ŒLLMè¯„ä¼°
        all_chunks_with_scores.sort(key=lambda x: x[1], reverse=True)
        top_chunks = all_chunks_with_scores[:top_k]

        logger.info(f"  æ€»å…±æ”¶é›†äº† {len(all_chunks_with_scores)} ä¸ªchunks")
        logger.info(f"  æŒ‰åˆ†æ•°æ’åºåå–å‰ {len(top_chunks)} ä¸ªè¿›è¡ŒLLMè¯„ä¼°")

        # æå–chunkså’Œæ¥æºæ˜ å°„
        all_chunks = [chunk for chunk, score, doc_name in top_chunks]
        chunk_to_source = {i: doc_name for i, (chunk, score, doc_name) in enumerate(top_chunks)}

        # Step 2: LLMæ‰¹é‡è¯„ä¼°ç›¸å…³æ€§
        relevant_chunks, eval_stats = await self.relevance_evaluator.batch_evaluate_relevance(
            question=question,
            chunks=all_chunks,
            batch_size=50
        )

        fallback_used = False

        # Step 3: ä¿åº•ç­–ç•¥ - å¦‚æœæ²¡æœ‰ç›¸å…³chunk
        if not relevant_chunks:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³chunkï¼Œå¯åŠ¨ä¿åº•ç­–ç•¥")
            fallback_used = True

            # Rerankæ‰€æœ‰chunks

            try:
                # ä½¿ç”¨rerankè¿›ä¸€æ­¥ç­›é€‰
                reranker = self.kb_manager.rerank_manager.get_reranker()

                if reranker:
                    # Rerankæ‰€æœ‰chunks
                    reranked = reranker.compress_documents(all_chunks, question)

                    # å–å‰k*fallback_ratioä¸ª
                    fallback_count = max(1, int(top_k * fallback_ratio))
                    relevant_chunks = reranked[:fallback_count]

                    logger.info(f"  ä¿åº•ç­–ç•¥: rerankåå–å‰ {len(relevant_chunks)} ä¸ªchunks")
                else:
                    # å¦‚æœæ²¡æœ‰rerankerï¼Œç›´æ¥å–å‰k*fallback_ratioä¸ª
                    fallback_count = max(1, int(top_k * fallback_ratio))
                    relevant_chunks = all_chunks[:fallback_count]
                    logger.info(f"  ä¿åº•ç­–ç•¥: ç›´æ¥å–å‰ {len(relevant_chunks)} ä¸ªchunks")

            except Exception as e:
                logger.error(f"ä¿åº•ç­–ç•¥å¤±è´¥: {e}")
                # æœ€åçš„ä¿åº•ï¼šç›´æ¥å–åŸå§‹top-kçš„ä¸€åŠ
                fallback_count = max(1, int(top_k * fallback_ratio))
                relevant_chunks = all_chunks[:fallback_count]

        # Format sources
        sources = []
        for i, chunk in enumerate(relevant_chunks, 1):
            # æ‰¾åˆ°chunkå¯¹åº”çš„æºæ–‡æ¡£
            chunk_id = all_chunks.index(chunk) if chunk in all_chunks else -1
            source_doc = chunk_to_source.get(chunk_id, "Unknown")

            source = {
                'index': i,
                'document': source_doc,
                'content': chunk.page_content,
                'metadata': chunk.metadata
            }
            if 'page' in chunk.metadata:
                source['page'] = chunk.metadata['page']
            sources.append(source)

        return {
            'relevant_chunks': relevant_chunks,
            'sources': sources,
            'stats': eval_stats,
            'fallback_used': fallback_used
        }