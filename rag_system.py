"""
RAG System Core
Main interface for the document analysis assistant
æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒ
"""
from typing import Optional, Iterator, Dict, Any, List
from loguru import logger

from config import SystemConfig, AvailableModels
from llm_client import LLMClientManager, EmbeddingManager, RerankManager
from knowledge_base import KnowledgeBaseManager
from conversation import ConversationManager


class DocumentAssistant:
    """
    Unified RAG System Interface
    Provides high-level API for document-based question answering
    """

    def __init__(self, config: Optional[SystemConfig] = None):
        """
        Initialize Document Assistant

        Args:
            config: System configuration (uses default if None)
        """
        self.config = config or SystemConfig.default()
        logger.info("Initializing Document Assistant...")

        # Initialize managers
        self.llm_manager = LLMClientManager(self.config.model)
        self.embedding_manager = EmbeddingManager(self.config.model.api_key)
        self.rerank_manager = RerankManager(self.config.model.api_key)

        # Initialize knowledge base
        self.kb_manager = KnowledgeBaseManager(
            self.embedding_manager,
            self.rerank_manager,
            self.config.vector_store,
            self.config.retrieval
        )

        # Initialize conversation
        self.conversation_manager = ConversationManager(
            self.llm_manager,
            self.kb_manager
        )

        logger.info("Document Assistant initialized successfully")

        # é¢„åŠ è½½æ‰€æœ‰æ–‡æ¡£åˆ°cacheï¼ˆé‡è¦ä¼˜åŒ–ï¼‰
        self._preload_documents()

    # ========== Document Management ==========

    def upload_document(self, file_path: str, original_filename: str = None) -> bool:
        """
        Upload and index a document

        Args:
            file_path: Path to document
            original_filename: Original filename (for proper encoding)

        Returns:
            True if successful
        """
        result = self.kb_manager.upload_document(file_path, original_filename=original_filename)
        return result is not None

    def list_documents(self) -> List[str]:
        """
        List all indexed documents

        Returns:
            List of document names
        """
        return self.kb_manager.list_documents()

    def delete_document(self, document_name: str) -> bool:
        """
        Delete a document

        Args:
            document_name: Document to delete

        Returns:
            True if successful
        """
        return self.kb_manager.delete_document(document_name)

    def update_document(self, file_path: str, force: bool = False) -> bool:
        """
        Update an existing document

        Args:
            file_path: Path to the updated document
            force: Force update even if content hasn't changed

        Returns:
            True if successful
        """
        result = self.kb_manager.update_document(file_path, force)
        return result is not None

    def get_document_info(self, document_name: str) -> Optional[Dict[str, Any]]:
        """
        Get detailed information about a document

        Args:
            document_name: Name of the document

        Returns:
            Dictionary with document information
        """
        return self.kb_manager.get_document_info(document_name)

    # ========== Conversation ==========

    def ask(
            self,
            question: str,
            document_name: Optional[str] = None
    ) -> str:
        """
        Ask a question (synchronous)

        Args:
            question: User question
            document_name: Optional document for context

        Returns:
            Answer text
        """
        response = self.conversation_manager.chat(question, document_name)
        return response.get('answer', '')

    def ask_stream(
            self,
            question: str,
            document_name: Optional[str] = None
    ) -> Iterator[str]:
        """
        Ask a question with streaming response

        Args:
            question: User question
            document_name: Optional document for context

        Yields:
            Answer chunks
        """
        for chunk in self.conversation_manager.chat_stream(question, document_name):
            if 'answer' in chunk and chunk['answer']:
                yield chunk['answer']

    def clear_conversation(self) -> None:
        """Clear conversation history"""
        self.conversation_manager.clear_history()

    def get_conversation_history(self) -> list:
        """Get conversation history"""
        return self.conversation_manager.get_history()

    # ========== Model Management ==========

    def switch_model(self, model_name: str) -> bool:
        """
        Switch to a different model

        Args:
            model_name: New model name

        Returns:
            True if successful
        """
        try:
            if model_name not in AvailableModels.all():
                logger.error(f"Unknown model: {model_name}")
                return False

            self.llm_manager.update_model(model_name)
            logger.info(f"Switched to model: {model_name}")
            return True
        except Exception as e:
            logger.error(f"Failed to switch model: {e}")
            return False

    def update_parameters(
            self,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None
    ) -> None:
        """
        Update model parameters

        Args:
            temperature: Temperature value
            max_tokens: Maximum tokens
        """
        self.llm_manager.update_parameters(temperature, max_tokens)

    def get_current_model(self) -> str:
        """Get current model name"""
        return self.config.model.model_name

    def get_available_models(self) -> List[str]:
        """Get list of available models"""
        return AvailableModels.all()

    def ask_all_documents(
            self,
            question: str
    ) -> Dict[str, Any]:
        """
        Ask question across ALL documents in knowledge base

        Args:
            question: User question

        Returns:
            Dictionary with answer and sources from all documents
        """
        logger.info("Querying all documents in knowledge base")

        # Get all documents
        all_docs = self.list_documents()

        if not all_docs:
            return {
                'answer': "çŸ¥è¯†åº“ä¸­æ²¡æœ‰ä»»ä½•æ–‡æ¡£ã€‚è¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚",
                'sources': [],
                'num_sources': 0
            }

        logger.info(f"Found {len(all_docs)} documents to search")

        # Collect contexts from all documents
        all_contexts = []

        for doc_name in all_docs:
            retriever = self.kb_manager.get_retriever(doc_name)
            if retriever:
                try:
                    docs = retriever.get_relevant_documents(question)
                    for doc in docs:
                        all_contexts.append({
                            'source': doc_name,
                            'content': doc.page_content,
                            'metadata': doc.metadata
                        })
                    logger.info(f"Retrieved {len(docs)} chunks from {doc_name}")
                except Exception as e:
                    logger.error(f"Failed to retrieve from {doc_name}: {e}")

        if not all_contexts:
            return {
                'answer': "æœªåœ¨ä»»ä½•æ–‡æ¡£ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                'sources': [],
                'num_sources': 0
            }

        logger.info(f"Total retrieved chunks: {len(all_contexts)}")

        # Build enhanced prompt with all contexts
        context_parts = []
        for i, ctx in enumerate(all_contexts, 1):
            page_info = f" (ç¬¬{ctx['metadata'].get('page', '?')}é¡µ)" if 'page' in ctx['metadata'] else ""
            context_parts.append(
                f"[æ¥æº {i}ï¼š{ctx['source']}{page_info}]\n{ctx['content']}"
            )

        context_text = "\n\n---\n\n".join(context_parts)

        enhanced_question = f"""åŸºäºä»¥ä¸‹ä»çŸ¥è¯†åº“æ‰€æœ‰æ–‡æ¡£ä¸­æ£€ç´¢åˆ°çš„ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

é—®é¢˜ï¼š{question}

æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼š

{context_text}

---

è¯·åŸºäºä¸Šè¿°æ£€ç´¢åˆ°çš„å†…å®¹å›ç­”é—®é¢˜ï¼š
1. ä»…ä½¿ç”¨ä¸Šè¿°æ–‡æ¡£ä¸­çš„äº‹å®ä¿¡æ¯
2. æ˜ç¡®æ ‡æ³¨ä¿¡æ¯æ¥è‡ªå“ªä¸ªæ–‡æ¡£
3. å¦‚æœä¿¡æ¯è·¨è¶Šå¤šä¸ªæ–‡æ¡£ï¼Œè¯·ç»¼åˆåˆ†æ
4. ä¸è¦æ¨æµ‹æ–‡æ¡£ä¸­æ²¡æœ‰çš„å†…å®¹
5. æŒ‰æ—¶é—´é¡ºåºæˆ–é€»è¾‘é¡ºåºç»„ç»‡ä¿¡æ¯ï¼ˆå¦‚æœé€‚ç”¨ï¼‰"""

        # ä¿ç•™å¯¹è¯å†å²ï¼Œæ”¯æŒä¸Šä¸‹æ–‡è®°å¿†
        # self.clear_conversation()  # âŒ ç§»é™¤æ¸…ç©ºå†å²

        # Query without specific document (direct LLM call)
        response = self.conversation_manager.chat(enhanced_question, document_name=None)

        # Format sources
        sources = []
        for i, ctx in enumerate(all_contexts, 1):
            source = {
                'index': i,
                'document': ctx['source'],
                'content': ctx['content'],
                'metadata': ctx['metadata']
            }
            if 'page' in ctx['metadata']:
                source['page'] = ctx['metadata']['page']
            sources.append(source)

        return {
            'answer': response.get('answer', ''),
            'sources': sources,
            'num_sources': len(sources),
            'documents_searched': len(all_docs)
        }

    def ask_all_documents_stream(
            self,
            question: str
    ) -> Iterator[Dict[str, Any]]:
        """
        æ™ºèƒ½ä»»åŠ¡åˆ†ç±»çš„å…¨å±€æ£€ç´¢

        æ ¹æ®é—®é¢˜ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ£€ç´¢ç­–ç•¥ï¼š
        1. ç»Ÿè®¡å‹ï¼ˆæäº†å‡ æ¬¡ï¼‰â†’ é«˜å¬å›top20
        2. æ¼”å˜å‹ï¼ˆè§‚ç‚¹å˜åŒ–ï¼‰â†’ åˆ†é˜¶æ®µæ£€ç´¢
        3. ä¸€èˆ¬å‹ï¼ˆç›´æ¥é—®ç­”ï¼‰â†’ ç²¾ç¡®æ£€ç´¢top5
        """
        logger.info("å¯åŠ¨æ™ºèƒ½æ£€ç´¢æ¨¡å¼")

        # è·å–æ‰€æœ‰æ–‡æ¡£
        all_docs = self.list_documents()

        if not all_docs:
            yield {
                'answer': "çŸ¥è¯†åº“ä¸­æ²¡æœ‰ä»»ä½•æ–‡æ¡£ã€‚è¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚",
                'sources': [],
                'done': True
            }
            return

        # Step 1: ä»»åŠ¡åˆ†ç±»
        task_type = self._classify_task(question)
        logger.info(f"ä»»åŠ¡ç±»å‹è¯†åˆ«ä¸º: {task_type}")

        # Step 2: æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒç”¨ä¸åŒç­–ç•¥
        if task_type == "STATISTICAL":
            yield from self._statistical_query(question, all_docs)
        elif task_type == "EVOLUTION":
            yield from self._evolution_query(question, all_docs)
        else:
            yield from self._general_query(question, all_docs)

    def _classify_task(self, question: str) -> str:
        """
        è¯†åˆ«ä»»åŠ¡ç±»å‹

        Returns:
            STATISTICAL: ç»Ÿè®¡å‹ï¼ˆè®¡æ•°ã€é¢‘ç‡ï¼‰
            EVOLUTION: æ¼”å˜å‹ï¼ˆè§‚ç‚¹å˜åŒ–ï¼‰
            GENERAL: ä¸€èˆ¬å‹ï¼ˆç›´æ¥é—®ç­”ï¼‰
        """
        q_lower = question.lower()

        # ç»Ÿè®¡å‹å…³é”®è¯
        statistical_keywords = [
            'å¤šå°‘æ¬¡', 'é¢‘ç‡', 'å‡ºç°', 'æåŠ', 'æ¬¡æ•°', 'æäº†', 'è¯´äº†',
            'å‡ æ¬¡', 'ç»Ÿè®¡', 'è®¡æ•°', 'åˆ—ä¸¾', 'æ‰€æœ‰'
        ]

        # æ¼”å˜å‹å…³é”®è¯
        evolution_keywords = [
            'å˜åŒ–', 'ä»', 'åˆ°', 'è½¬å˜', 'æ¼”å˜', 'å‘å±•', 'è¶‹åŠ¿',
            'ä¸ºä»€ä¹ˆ', 'åŸå› ', 'æ€åº¦', 'çœ‹æ³•å˜', 'è§‚ç‚¹å˜'
        ]

        # æ£€æµ‹ç»Ÿè®¡å‹
        if any(kw in q_lower for kw in statistical_keywords):
            return "STATISTICAL"

        # æ£€æµ‹æ¼”å˜å‹ï¼ˆåŒæ—¶åŒ…å«å¯¹æ¯”è¯ï¼‰
        if any(kw in q_lower for kw in evolution_keywords):
            # è¿›ä¸€æ­¥æ£€æµ‹æ˜¯å¦æœ‰å¯¹æ¯”ç»“æ„
            if ('ä»' in q_lower and 'åˆ°' in q_lower) or 'å˜åŒ–' in q_lower:
                return "EVOLUTION"

        return "GENERAL"

    def _statistical_query(
            self,
            question: str,
            all_docs: List[str]
    ) -> Iterator[Dict[str, Any]]:
        """
        ç»Ÿè®¡å‹æŸ¥è¯¢ï¼šé«˜å¬å›ç‡ç­–ç•¥
        ç›®æ ‡ï¼šå°½å¯èƒ½æ‰¾åˆ°æ‰€æœ‰ç›¸å…³mentions
        """
        logger.info("ğŸ“Š ç»Ÿè®¡å‹æŸ¥è¯¢ï¼šä½¿ç”¨é«˜å¬å›æ¨¡å¼ï¼ˆtop20ï¼‰")

        # æ”¶é›†æ‰€æœ‰chunksï¼ˆé¢„åŠ è½½å·²å®Œæˆï¼Œç›´æ¥ä»cacheè·å–ï¼‰
        all_chunks = []
        for doc_name in all_docs:
            from vector_store import generate_collection_id
            collection_id = generate_collection_id(doc_name)

            # ä»cacheè·å–ï¼ˆå¦‚æœæ²¡æœ‰åˆ™è·³è¿‡å¹¶è­¦å‘Šï¼‰
            if collection_id in self.kb_manager.document_cache:
                chunks = self.kb_manager.document_cache[collection_id]
                for chunk in chunks:
                    chunk.metadata['source_document'] = doc_name
                    all_chunks.append(chunk)
            else:
                logger.warning(f"æ–‡æ¡£ {doc_name} ä¸åœ¨cacheä¸­ï¼ˆé¢„åŠ è½½å¯èƒ½å¤±è´¥ï¼‰")

        if not all_chunks:
            yield {'answer': "æ— æ³•åŠ è½½æ–‡æ¡£å†…å®¹ã€‚è¯·ç¡®ä¿æ–‡æ¡£å·²æ­£ç¡®ä¸Šä¼ ã€‚", 'sources': [], 'done': True}
            return

        logger.info(f"æ”¶é›†åˆ° {len(all_chunks)} ä¸ªchunks")

        # ä½¿ç”¨BM25æ£€ç´¢ï¼Œé¿å…SSLé”™è¯¯
        from langchain_community.retrievers import BM25Retriever

        try:
            # é«˜å¬å›ï¼šä½¿ç”¨BM25æ£€ç´¢top20ï¼ˆä¸ä¾èµ–embeddingï¼‰
            bm25_retriever = BM25Retriever.from_documents(all_chunks, k=20)
            retrieved = bm25_retriever.invoke(question)
            logger.info(f"âœ… BM25å¬å› {len(retrieved)} ä¸ªç›¸å…³chunks")

        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
            yield {'answer': f"âŒ æ£€ç´¢å¤±è´¥: {str(e)}", 'sources': [], 'done': True}
            return

        # æŒ‰æ–‡æ¡£åˆ†ç»„å¹¶æ’åº
        contexts_by_doc = {}
        for chunk in retrieved:
            doc = chunk.metadata.get('source_document', 'unknown')
            if doc not in contexts_by_doc:
                contexts_by_doc[doc] = []
            contexts_by_doc[doc].append(chunk)

        # æ„å»ºç»“æ„åŒ–ä¸Šä¸‹æ–‡
        context_parts = []
        for doc_name in sorted(contexts_by_doc.keys()):
            context_parts.append(f"\n{'=' * 60}\næ–‡æ¡£ï¼š{doc_name}\n{'=' * 60}")
            for i, chunk in enumerate(contexts_by_doc[doc_name], 1):
                page = chunk.metadata.get('page', '?')
                context_parts.append(f"\n[ç‰‡æ®µ{i}ï¼Œç¬¬{page}é¡µ]\n{chunk.page_content}")

        context_text = "\n".join(context_parts)

        # ç»Ÿè®¡å‹prompt
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„æŠ¥å‘Šç ”ç©¶åˆ†æå¸ˆã€‚åŸºäºæ£€ç´¢åˆ°çš„åŸæ–‡ç‰‡æ®µå›ç­”ç»Ÿè®¡ç±»é—®é¢˜ã€‚

é—®é¢˜ï¼š{question}

æ£€ç´¢ç»“æœï¼ˆå·²æŒ‰æ–‡æ¡£åˆ†ç»„ï¼Œå…±{len(contexts_by_doc)}ä¸ªæ–‡æ¡£ï¼Œ{len(retrieved)}ä¸ªç›¸å…³ç‰‡æ®µï¼‰ï¼š

{context_text}

---

ã€å›ç­”è¦æ±‚ã€‘
1. **ç»Ÿè®¡å‡†ç¡®**ï¼šä»”ç»†é˜…è¯»æ¯ä¸ªç‰‡æ®µï¼Œé€ä¸€è®¡æ•°ï¼Œä¸è¦é—æ¼ä»»ä½•æåŠ
2. **æ ‡æ³¨å‡ºå¤„**ï¼šæ¯æ¬¡æåŠéƒ½å¿…é¡»æ ‡æ³¨ [æ–‡æ¡£å-ç¬¬Xé¡µ]ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰æ ‡æ³¨é¡µæ•°ï¼Œä»…æ ‡æ³¨[æ–‡æ¡£å]å³å¯
3. **æ—¶é—´æ’åº**ï¼šæŒ‰å¹´ä»½é¡ºåºåˆ—ä¸¾ï¼ˆå¦‚æœæ–‡æ¡£ååŒ…å«å¹´ä»½ï¼‰
4. **åŸæ–‡å¼•ç”¨**ï¼šå…³é”®è§‚ç‚¹è¦å¼•ç”¨åŸæ–‡ç‰‡æ®µ
5. **æ€»ç»“ç»Ÿè®¡**ï¼šæœ€åç»™å‡ºæ˜ç¡®çš„ç»Ÿè®¡ç»“æœ

ã€å›ç­”æ ¼å¼ç¤ºä¾‹ã€‘
æ ¹æ®æ£€ç´¢åˆ°çš„{len(retrieved)}ä¸ªç›¸å…³ç‰‡æ®µï¼Œç»Ÿè®¡ç»“æœå¦‚ä¸‹ï¼š

1. [1957å¹´ä¿¡-ç¬¬2é¡µ] é¦–æ¬¡æåŠï¼Œè§‚ç‚¹ï¼š...
   åŸæ–‡ï¼š"..."

2. [1958å¹´ä¿¡] å†æ¬¡è®¨è®ºï¼Œè§‚ç‚¹ï¼š...
   åŸæ–‡ï¼š"..."

...

ã€ç»Ÿè®¡æ€»ç»“ã€‘
- æ—¶é—´èŒƒå›´ï¼šXXXXå¹´-XXXXå¹´
- æåŠæ¬¡æ•°ï¼šXæ¬¡
- æ¶‰åŠæ–‡æ¡£ï¼šXä¸ª
"""

        # æµå¼ç”Ÿæˆï¼ˆä¿ç•™å¯¹è¯å†å²ï¼‰
        # self.clear_conversation()  # âŒ ç§»é™¤æ¸…ç©ºå†å²
        for chunk in self.conversation_manager.chat_stream(prompt, document_name=None):
            if 'answer' in chunk and chunk['answer']:
                yield {'answer': chunk['answer']}

        # è¿”å›æ‰€æœ‰sources
        sources = []
        for i, chunk in enumerate(retrieved, 1):
            source = {
                'index': i,
                'document': chunk.metadata.get('source_document', 'unknown'),
                'content': chunk.page_content,
                'metadata': chunk.metadata
            }
            if 'page' in chunk.metadata:
                source['page'] = chunk.metadata['page']
            sources.append(source)

        yield {
            'sources': sources,
            'num_sources': len(sources),
            'task_type': 'STATISTICAL',
            'recall_mode': 'high',
            'done': True
        }

    def _evolution_query(
            self,
            question: str,
            all_docs: List[str]
    ) -> Iterator[Dict[str, Any]]:
        """
        æ¼”å˜å‹æŸ¥è¯¢ï¼šæ—¶é—´åºåˆ—åˆ†æ
        ç›®æ ‡ï¼šå±•ç¤ºè§‚ç‚¹éšæ—¶é—´çš„å˜åŒ–
        """
        logger.info(" æ¼”å˜å‹æŸ¥è¯¢ï¼šä½¿ç”¨æ—¶é—´åºåˆ—åˆ†ææ¨¡å¼")
        logger.info(" æ¼”å˜å‹æŸ¥è¯¢ï¼šä½¿ç”¨æ—¶é—´åºåˆ—åˆ†ææ¨¡å¼")

        # æ”¶é›†æ‰€æœ‰chunksï¼ˆé¢„åŠ è½½å·²å®Œæˆï¼‰
        all_chunks = []
        for doc_name in all_docs:
            from vector_store import generate_collection_id
            collection_id = generate_collection_id(doc_name)

            # ä»cacheè·å–
            if collection_id in self.kb_manager.document_cache:
                chunks = self.kb_manager.document_cache[collection_id]
                for chunk in chunks:
                    chunk.metadata['source_document'] = doc_name
                    chunk.metadata['year'] = self._extract_year(doc_name)
                    all_chunks.append(chunk)
            else:
                logger.warning(f"æ–‡æ¡£ {doc_name} ä¸åœ¨cacheä¸­")

        logger.info(f"âœ… æ¼”å˜å‹æŸ¥è¯¢ï¼šæˆåŠŸæ”¶é›† {len(all_chunks)} ä¸ªchunks")

        if not all_chunks:
            yield {'answer': "æ— æ³•åŠ è½½æ–‡æ¡£å†…å®¹ã€‚è¯·ç¡®ä¿æ–‡æ¡£å·²æ­£ç¡®ä¸Šä¼ ã€‚", 'sources': [], 'done': True}
            return

        # ä½¿ç”¨BM25æ£€ç´¢ï¼Œé¿å…SSLé”™è¯¯
        from langchain_community.retrievers import BM25Retriever

        try:
            # æ£€ç´¢top15ï¼ˆä¸ä¾èµ–embeddingï¼‰
            bm25_retriever = BM25Retriever.from_documents(all_chunks, k=15)
            retrieved = bm25_retriever.invoke(question)
            logger.info(f"âœ… BM25æ£€ç´¢åˆ° {len(retrieved)} ä¸ªç›¸å…³chunks")

        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
            yield {'answer': f"âŒ æ£€ç´¢å¤±è´¥: {str(e)}", 'sources': [], 'done': True}
            return

        # æŒ‰æ—¶é—´æ’åº
        sorted_chunks = sorted(
            retrieved,
            key=lambda x: (x.metadata.get('year', '9999'), x.metadata.get('source_document', ''))
        )

        # æ„å»ºæ—¶é—´çº¿ä¸Šä¸‹æ–‡
        context_parts = []
        current_year = None

        for i, chunk in enumerate(sorted_chunks, 1):
            doc = chunk.metadata.get('source_document', 'unknown')
            year = chunk.metadata.get('year', '?')
            page = chunk.metadata.get('page', '?')

            # å¹´ä»½åˆ†éš”
            if year != current_year:
                current_year = year
                context_parts.append(f"\n{'=' * 60}\nã€{year}å¹´ã€‘\n{'=' * 60}")

            context_parts.append(f"\n[{doc}-ç¬¬{page}é¡µ]\n{chunk.page_content}")

        context_text = "\n".join(context_parts)

        # æ¼”å˜å‹prompt
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„æ–‡æ¡£åˆ†æå¸ˆã€‚åŸºäºæ£€ç´¢åˆ°çš„æŒ‰æ—¶é—´æ’åºçš„åŸæ–‡ç‰‡æ®µï¼Œåˆ†æè§‚ç‚¹æ¼”å˜è¿‡ç¨‹ã€‚

é—®é¢˜ï¼š{question}

æ£€ç´¢ç»“æœï¼ˆå·²æŒ‰æ—¶é—´æ’åºï¼Œå…±{len(sorted_chunks)}ä¸ªç‰‡æ®µï¼‰ï¼š

{context_text}

---

ã€å›ç­”è¦æ±‚ã€‘
1. **æ—¶é—´çº¿ç´¢**ï¼šæ¸…æ™°å±•ç¤ºè§‚ç‚¹éšæ—¶é—´çš„æ¼”å˜è½¨è¿¹
2. **é˜¶æ®µåˆ’åˆ†**ï¼šè¯†åˆ«å…³é”®è½¬æŠ˜ç‚¹ï¼Œåˆ’åˆ†ä¸åŒé˜¶æ®µ
3. **åŸå› åˆ†æ**ï¼šåˆ†ææ¯æ¬¡è½¬å˜çš„å¯èƒ½åŸå› æˆ–èƒŒæ™¯
4. **ç²¾ç¡®å¼•ç”¨**ï¼šæ¯ä¸ªè§‚ç‚¹éƒ½è¦æ ‡æ³¨ [æ–‡æ¡£å-ç¬¬Xé¡µ] å¹¶å¼•ç”¨åŸæ–‡ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰æ ‡æ³¨é¡µæ•°ï¼Œä»…æ ‡æ³¨[æ–‡æ¡£å]å³å¯
5. **å¯¹æ¯”åˆ†æ**ï¼šæ˜ç¡®æŒ‡å‡ºå‰åè§‚ç‚¹çš„å¼‚åŒ

ã€å›ç­”æ ¼å¼ç¤ºä¾‹ã€‘
è§‚ç‚¹æ¼”å˜åˆ†æï¼š

ä¸€ã€æ—©æœŸé˜¶æ®µï¼ˆXXXXå¹´-XXXXå¹´ï¼‰ï¼š[æ¦‚æ‹¬æ€§æè¿°]
   [XXXXå¹´ä¿¡-ç¬¬Xé¡µ] è§‚ç‚¹1ï¼š...
   åŸæ–‡å¼•ç”¨ï¼š"..."

   [XXXXå¹´ä¿¡-ç¬¬Xé¡µ] è§‚ç‚¹2ï¼š...
   åŸæ–‡å¼•ç”¨ï¼š"..."

äºŒã€è½¬æŠ˜ç‚¹ï¼ˆXXXXå¹´ï¼‰ï¼š
   [XXXXå¹´ä¿¡-ç¬¬Xé¡µ] å…³é”®å˜åŒ–ï¼š...
   å¯èƒ½åŸå› ï¼š...

ä¸‰ã€æ–°é˜¶æ®µï¼ˆXXXXå¹´-XXXXå¹´ï¼‰ï¼š[æ¦‚æ‹¬æ€§æè¿°]
   [XXXXå¹´ä¿¡-ç¬¬Xé¡µ] æ–°è§‚ç‚¹ï¼š...
   ä¸æ—©æœŸå¯¹æ¯”ï¼š...

ã€æ¼”å˜æ€»ç»“ã€‘
- ä¸»è¦å˜åŒ–ï¼š...
- å…³é”®è½¬æŠ˜ï¼š...
- æ·±å±‚åŸå› ï¼š...
"""

        # æµå¼ç”Ÿæˆï¼ˆä¿ç•™å¯¹è¯å†å²ï¼‰
        # self.clear_conversation()  # âŒ ç§»é™¤æ¸…ç©ºå†å²
        for chunk in self.conversation_manager.chat_stream(prompt, document_name=None):
            if 'answer' in chunk and chunk['answer']:
                yield {'answer': chunk['answer']}

        # è¿”å›sources
        sources = []
        for i, chunk in enumerate(sorted_chunks, 1):
            source = {
                'index': i,
                'document': chunk.metadata.get('source_document', 'unknown'),
                'year': chunk.metadata.get('year', '?'),
                'content': chunk.page_content,
                'metadata': chunk.metadata
            }
            if 'page' in chunk.metadata:
                source['page'] = chunk.metadata['page']
            sources.append(source)

        yield {
            'sources': sources,
            'num_sources': len(sources),
            'task_type': 'EVOLUTION',
            'done': True
        }

    def _general_query(
            self,
            question: str,
            all_docs: List[str]
    ) -> Iterator[Dict[str, Any]]:
        """
        ä¸€èˆ¬å‹æŸ¥è¯¢ï¼šç²¾ç¡®æ£€ç´¢
        ç›®æ ‡ï¼šç›´æ¥å›ç­”å…·ä½“é—®é¢˜
        """
        logger.info("ğŸ¯ ä¸€èˆ¬å‹æŸ¥è¯¢ï¼šä½¿ç”¨ç²¾ç¡®æ£€ç´¢æ¨¡å¼ï¼ˆtop5ï¼‰")

        # æ”¶é›†æ‰€æœ‰chunksï¼ˆé¢„åŠ è½½å·²å®Œæˆï¼‰
        all_chunks = []
        for doc_name in all_docs:
            from vector_store import generate_collection_id
            collection_id = generate_collection_id(doc_name)

            # ä»cacheè·å–
            if collection_id in self.kb_manager.document_cache:
                chunks = self.kb_manager.document_cache[collection_id]
                for chunk in chunks:
                    chunk.metadata['source_document'] = doc_name
                    all_chunks.append(chunk)
            else:
                logger.warning(f"æ–‡æ¡£ {doc_name} ä¸åœ¨cacheä¸­")

        if not all_chunks:
            yield {'answer': "æ— æ³•åŠ è½½æ–‡æ¡£å†…å®¹", 'sources': [], 'done': True}
            return

        logger.info(f"æ”¶é›†åˆ° {len(all_chunks)} ä¸ªchunks")

        # ä½¿ç”¨é¢„åŠ è½½çš„æ£€ç´¢å™¨ï¼Œè€Œä¸æ˜¯åˆ›å»ºä¸´æ—¶å‘é‡åº“ï¼ˆé¿å…SSLé”™è¯¯ï¼‰
        from langchain.retrievers import EnsembleRetriever
        from langchain_community.retrievers import BM25Retriever
        from langchain.schema import Document

        try:
            # æ–¹æ¡ˆï¼šä½¿ç”¨BM25 + ç®€å•æ’åºï¼Œä¸ä¾èµ–embedding
            # åˆ›å»ºBM25æ£€ç´¢å™¨
            bm25_retriever = BM25Retriever.from_documents(all_chunks, k=15)

            # ç›´æ¥ä½¿ç”¨BM25æ£€ç´¢ï¼ˆä¸ä½¿ç”¨å‘é‡æ£€ç´¢ï¼Œé¿å…SSLï¼‰
            retrieved = bm25_retriever.invoke(question)
            logger.info(f"âœ… BM25æ£€ç´¢è¿”å› {len(retrieved)} ä¸ªæœ€ç›¸å…³chunks")

            # ç®€å•æˆªæ–­åˆ°top10
            retrieved = retrieved[:10] if len(retrieved) > 10 else retrieved

        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
            yield {'answer': f"âŒ æ£€ç´¢å¤±è´¥: {str(e)}", 'sources': [], 'done': True}
            return

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, chunk in enumerate(retrieved, 1):
            doc = chunk.metadata.get('source_document', 'unknown')
            page = chunk.metadata.get('page', '?')
            context_parts.append(f"[æ¥æº{i}ï¼š{doc}-ç¬¬{page}é¡µ]\n{chunk.page_content}")

        context_text = "\n\n---\n\n".join(context_parts)

        # ä¸€èˆ¬å‹prompt
        prompt = f"""åŸºäºæ£€ç´¢åˆ°çš„æœ€ç›¸å…³æ–‡æ¡£ç‰‡æ®µå›ç­”é—®é¢˜ã€‚

é—®é¢˜ï¼š{question}

æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼ˆå·²æŒ‰ç›¸å…³åº¦æ’åºï¼‰ï¼š

{context_text}

---

è¯·åŸºäºä¸Šè¿°æ£€ç´¢åˆ°çš„å†…å®¹å›ç­”é—®é¢˜ï¼š
1. ä»…ä½¿ç”¨ä¸Šè¿°æ–‡æ¡£ä¸­çš„äº‹å®ä¿¡æ¯
2. æ˜ç¡®æ ‡æ³¨ä¿¡æ¯æ¥æº [æ–‡æ¡£å-ç¬¬Xé¡µ]ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰æ ‡æ³¨é¡µæ•°ï¼Œä»…æ ‡æ³¨[æ–‡æ¡£å]å³å¯
3. å…³é”®è§‚ç‚¹è¦å¼•ç”¨åŸæ–‡
4. ä¸è¦æ¨æµ‹æ–‡æ¡£ä¸­æ²¡æœ‰çš„å†…å®¹
"""

        # æµå¼ç”Ÿæˆï¼ˆä¿ç•™å¯¹è¯å†å²ï¼Œæ”¯æŒä¸Šä¸‹æ–‡è®°å¿†ï¼‰
        # self.clear_conversation()  # âŒ ç§»é™¤æ¸…ç©ºå†å²ï¼Œä¿ç•™çŸ­æœŸè®°å¿†
        for chunk in self.conversation_manager.chat_stream(prompt, document_name=None):
            if 'answer' in chunk and chunk['answer']:
                yield {'answer': chunk['answer']}

        # è¿”å›sources
        sources = []
        for i, chunk in enumerate(retrieved, 1):
            source = {
                'index': i,
                'document': chunk.metadata.get('source_document', 'unknown'),
                'content': chunk.page_content,
                'metadata': chunk.metadata
            }
            if 'page' in chunk.metadata:
                source['page'] = chunk.metadata['page']
            sources.append(source)

        yield {
            'sources': sources,
            'num_sources': len(sources),
            'task_type': 'GENERAL',
            'done': True
        }

    def _extract_year(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæå–å¹´ä»½"""
        import re
        match = re.search(r'(19\d{2}|20\d{2})', filename)
        return match.group(1) if match else '9999'

    def _preload_documents(self) -> None:
        """
        é¢„åŠ è½½æ‰€æœ‰æ–‡æ¡£åˆ°cache
        ä¼˜åŒ–é¦–æ¬¡æŸ¥è¯¢æ€§èƒ½
        """
        all_docs = self.list_documents()
        if not all_docs:
            logger.info("çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— éœ€é¢„åŠ è½½")
            return

        logger.info(f"ğŸš€ é¢„åŠ è½½ {len(all_docs)} ä¸ªæ–‡æ¡£åˆ°cache...")
        loaded_count = 0

        for doc_name in all_docs:
            from vector_store import generate_collection_id
            collection_id = generate_collection_id(doc_name)

            # åªåŠ è½½ä¸åœ¨cacheä¸­çš„
            if collection_id not in self.kb_manager.document_cache:
                retriever = self.kb_manager.get_retriever(doc_name)
                if retriever and collection_id in self.kb_manager.document_cache:
                    chunks = len(self.kb_manager.document_cache[collection_id])
                    logger.info(f"  âœ… {doc_name}: {chunks} chunks")
                    loaded_count += 1
                else:
                    logger.warning(f"  âš ï¸ {doc_name}: åŠ è½½å¤±è´¥")

        logger.info(f"âœ… é¢„åŠ è½½å®Œæˆ: {loaded_count}/{len(all_docs)} ä¸ªæ–‡æ¡£")

    def ask_all_documents_smart_stream(
        self,
        question: str,
        top_k: int = 10,
        fallback_ratio: float = 0.5
    ) -> Iterator[Dict[str, Any]]:
        """
        æ™ºèƒ½æ¨¡å¼å…¨æ–‡æ¡£æ£€ç´¢ï¼ˆæµå¼è¾“å‡ºï¼‰
        å¸¦LLMç›¸å…³æ€§è¿‡æ»¤çš„å…¨æ–‡æ¡£æ£€ç´¢

        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: å‘é‡æ£€ç´¢çš„top-k
            fallback_ratio: ä¿åº•æ¯”ä¾‹ï¼ˆ0-1ï¼‰

        Yields:
            æµå¼è¾“å‡ºå­—å…¸
        """
        logger.info(f"ğŸ§  å¯åŠ¨æ™ºèƒ½æ¨¡å¼å…¨æ–‡æ¡£æ£€ç´¢ (top_k={top_k}, fallback_ratio={fallback_ratio})")

        # è·å–æ‰€æœ‰æ–‡æ¡£
        all_docs = self.list_documents()

        if not all_docs:
            yield {
                'answer': "çŸ¥è¯†åº“ä¸­æ²¡æœ‰ä»»ä½•æ–‡æ¡£ã€‚è¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚",
                'sources': [],
                'done': True
            }
            return

        # æç¤ºå¼€å§‹æ£€ç´¢
        yield {'answer': 'ğŸ” æ­£åœ¨ä»æ‰€æœ‰æ–‡æ¡£ä¸­æ£€ç´¢ç›¸å…³å†…å®¹...\n\n', 'sources': []}

        # ä½¿ç”¨asyncioè¿è¡Œå¼‚æ­¥æ£€ç´¢
        import asyncio

        try:
            # åˆ›å»ºäº‹ä»¶å¾ªç¯
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            # æ‰§è¡Œæ™ºèƒ½æ£€ç´¢
            result = loop.run_until_complete(
                self.conversation_manager.smart_all_documents_retrieve(
                    question=question,
                    all_docs=all_docs,
                    top_k=top_k,
                    fallback_ratio=fallback_ratio
                )
            )

            loop.close()

        except Exception as e:
            logger.error(f"æ™ºèƒ½æ£€ç´¢å¤±è´¥: {e}")
            yield {
                'answer': f"æ£€ç´¢å¤±è´¥: {str(e)}",
                'sources': [],
                'done': True
            }
            return

        relevant_chunks = result['relevant_chunks']
        sources = result['sources']
        stats = result['stats']
        fallback_used = result['fallback_used']

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        if fallback_used:
            status_msg = f"âš ï¸ LLMè¯„ä¼°æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œä½¿ç”¨ä¿åº•ç­–ç•¥è·å¾— {len(relevant_chunks)} ä¸ªchunks\n\n"
        else:
            status_msg = f"âœ… LLMè¯„ä¼°å®Œæˆ: {stats['relevant']}/{stats['total']} ä¸ªç›¸å…³chunks\n\n"

        yield {'answer': status_msg, 'sources': []}

        if not relevant_chunks:
            yield {
                'answer': "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚",
                'sources': [],
                'done': True
            }
            return

        # æ„å»ºå¢å¼ºprompt
        context_parts = []
        for i, source in enumerate(sources, 1):
            page_info = f" (ç¬¬{source.get('page', '?')}é¡µ)" if 'page' in source else ""
            context_parts.append(
                f"[æ¥æº {i}ï¼š{source['document']}{page_info}]\n{source['content']}"
            )

        context_text = "\n\n---\n\n".join(context_parts)

        enhanced_question = f"""åŸºäºä»¥ä¸‹ç»è¿‡LLMç›¸å…³æ€§è¯„ä¼°åçš„é«˜è´¨é‡å†…å®¹å›ç­”é—®é¢˜ã€‚

é—®é¢˜ï¼š{question}

ç»è¿‡æ™ºèƒ½ç­›é€‰çš„ç›¸å…³å†…å®¹ï¼š

{context_text}

---

è¯·åŸºäºä¸Šè¿°å†…å®¹å›ç­”é—®é¢˜ï¼š
1. ä»…ä½¿ç”¨ä¸Šè¿°æ–‡æ¡£ä¸­çš„äº‹å®ä¿¡æ¯
2. æ˜ç¡®æ ‡æ³¨ä¿¡æ¯æ¥è‡ªå“ªä¸ªæ–‡æ¡£
3. å¦‚æœä¿¡æ¯è·¨è¶Šå¤šä¸ªæ–‡æ¡£ï¼Œè¯·ç»¼åˆåˆ†æ
4. ä¸è¦æ¨æµ‹æ–‡æ¡£ä¸­æ²¡æœ‰çš„å†…å®¹"""

        # ä¿ç•™å¯¹è¯å†å²ï¼Œæ”¯æŒä¸Šä¸‹æ–‡è®°å¿†
        # self.clear_conversation()  # âŒ ç§»é™¤æ¸…ç©ºå†å²ï¼Œä¿ç•™çŸ­æœŸè®°å¿†

        # æµå¼ç”Ÿæˆç­”æ¡ˆ
        for chunk in self.conversation_manager.chat_stream(enhanced_question, document_name=None):
            if 'answer' in chunk and chunk['answer']:
                yield {'answer': chunk['answer'], 'sources': []}

        # æœ€åè¿”å›sources
        yield {
            'answer': '',
            'sources': sources,
            'done': True,
            'metadata': {
                'total_chunks': stats['total'],
                'relevant_chunks': stats['relevant'],
                'irrelevant_chunks': stats['irrelevant'],
                'fallback_used': fallback_used,
                'documents_searched': len(all_docs)
            }
        }

    def get_status(self) -> Dict[str, Any]:
        """
        Get system status information

        Returns:
            Dictionary with status info
        """
        return {
            "model": self.get_current_model(),
            "documents": len(self.list_documents()),
            "conversation_turns": len(self.conversation_manager.get_history()) // 2,
            "retrieval_config": {
                "use_rerank": self.config.retrieval.use_rerank,
                "top_k": self.config.retrieval.top_k,
            }
        }


class MultiDocumentAssistant(DocumentAssistant):
    """
    Extended assistant with multi-document query support
    """

    def ask_multi_documents(
            self,
            question: str,
            document_names: List[str]
    ) -> str:
        """
        Ask question across multiple documents

        Args:
            question: User question
            document_names: List of documents to query

        Returns:
            Synthesized answer
        """
        logger.info(f"Querying {len(document_names)} documents")

        all_contexts = []

        # Retrieve from all documents
        for doc_name in document_names:
            retriever = self.kb_manager.get_retriever(doc_name)
            if retriever:
                try:
                    docs = retriever.get_relevant_documents(question)
                    for doc in docs:
                        all_contexts.append({
                            'source': doc_name,
                            'content': doc.page_content
                        })
                    logger.info(f"Retrieved {len(docs)} chunks from {doc_name}")
                except Exception as e:
                    logger.error(f"Failed to retrieve from {doc_name}: {e}")

        if not all_contexts:
            return "No relevant information found in the specified documents."

        # Build enhanced prompt
        context_text = "\n\n---\n\n".join([
            f"[Source: {ctx['source']}]\n{ctx['content']}"
            for ctx in all_contexts
        ])

        enhanced_question = f"""Based on the following information retrieved from multiple documents, answer the question.

Question: {question}

Retrieved Information:
{context_text}

---

Please answer based only on the information provided above. Clearly distinguish between different sources and do not speculate beyond what's in the documents."""

        # ä¿ç•™å¯¹è¯å†å²ï¼Œæ”¯æŒä¸Šä¸‹æ–‡è®°å¿†
        # self.clear_conversation()  # âŒ ç§»é™¤æ¸…ç©ºå†å²

        # Query without retrieval (we already have the context)
        return self.ask(enhanced_question, document_name=None)